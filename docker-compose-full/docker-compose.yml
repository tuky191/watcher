#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

version: '3.4'

networks:
  pulsar:
    driver: bridge

services:
  zk1:
    container_name: zk1
    hostname: zk1
    image: apachepulsar/pulsar-all:latest
    command: >
      bash -c "bin/apply-config-from-env.py conf/zookeeper.conf && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/generate-zookeeper-config.sh conf/zookeeper.conf && \
               exec bin/pulsar zookeeper"
    environment:
      ZOOKEEPER_SERVERS: zk1,zk2,zk3
    volumes:
      - pulsardatazk1:/pulsar/data
      - pulsarconfzk1:/pulsar/conf
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    networks:
      pulsar:

  zk2:
    container_name: zk2
    hostname: zk2
    image: apachepulsar/pulsar-all:latest
    command: >
      bash -c "bin/apply-config-from-env.py conf/zookeeper.conf && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/generate-zookeeper-config.sh conf/zookeeper.conf && \
               exec bin/pulsar zookeeper"
    environment:
      ZOOKEEPER_SERVERS: zk1,zk2,zk3
    volumes:
      - pulsardatazk2:/pulsar/data
      - pulsarconfzk2:/pulsar/conf
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    networks:
      pulsar:

  zk3:
    container_name: zk3
    hostname: zk3
    image: apachepulsar/pulsar-all:latest
    command: >
      bash -c "bin/apply-config-from-env.py conf/zookeeper.conf && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/generate-zookeeper-config.sh conf/zookeeper.conf && \
               exec bin/pulsar zookeeper"
    environment:
      ZOOKEEPER_SERVERS: zk1,zk2,zk3
    volumes:
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
      - pulsardatazk3:/pulsar/data
      - pulsarconfzk3:/pulsar/conf
    networks:
      pulsar:

  pulsar-init:
    container_name: pulsar-init
    hostname: pulsar-init
    image: apachepulsar/pulsar-all:latest
    command: bin/init-cluster.sh
    environment:
      clusterName: test
      zkServers: zk1:2181
      configurationStore: zk1:2181
      pulsarNode: proxy1
    volumes:
      - ./scripts/init-cluster.sh/:/pulsar/bin/init-cluster.sh
    depends_on:
      - zk1
      - zk2
      - zk3
    networks:
      pulsar:

  bk1:
    hostname: bk1
    container_name: bk1
    image: apachepulsar/pulsar-all:latest
    command: >
      bash -c "export dbStorage_writeCacheMaxSizeMb="$${dbStorage_writeCacheMaxSizeMb:-16}" && \
               export dbStorage_readAheadCacheMaxSizeMb="$${dbStorage_readAheadCacheMaxSizeMb:-16}" && \
               bin/apply-config-from-env.py conf/bookkeeper.conf && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/watch-znode.py -z $$zkServers -p /initialized-$$clusterName -w && \
               exec bin/pulsar bookie"
    environment:
      clusterName: test
      zkServers: zk1:2181,zk2:2181,zk3:2181
      numAddWorkerThreads: 8
      useHostNameAsBookieID: 'true'
    volumes:
      - pulsardatabk1:/pulsar/data
      - pulsarconfbk1:/pulsar/conf
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
    networks:
      pulsar:

  bk2:
    hostname: bk2
    container_name: bk2
    image: apachepulsar/pulsar-all:latest
    command: >
      bash -c "export dbStorage_writeCacheMaxSizeMb="${dbStorage_writeCacheMaxSizeMb:-16}" && \
               export dbStorage_readAheadCacheMaxSizeMb="${dbStorage_readAheadCacheMaxSizeMb:-16}" && \
               bin/apply-config-from-env.py conf/bookkeeper.conf && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/watch-znode.py -z $$zkServers -p /initialized-$$clusterName -w && \
               exec bin/pulsar bookie"
    environment:
      clusterName: test
      zkServers: zk1:2181,zk2:2181,zk3:2181
      numAddWorkerThreads: 8
      useHostNameAsBookieID: 'true'
    volumes:
      - pulsardatabk2:/pulsar/data
      - pulsarconfbk2:/pulsar/conf
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
    networks:
      pulsar:

  bk3:
    hostname: bk3
    container_name: bk3
    image: apachepulsar/pulsar-all:latest
    command: >
      bash -c "export dbStorage_writeCacheMaxSizeMb="${dbStorage_writeCacheMaxSizeMb:-16}" && \
               export dbStorage_readAheadCacheMaxSizeMb="${dbStorage_readAheadCacheMaxSizeMb:-16}" && \
               bin/apply-config-from-env.py conf/bookkeeper.conf && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/watch-znode.py -z $$zkServers -p /initialized-$$clusterName -w && \
               exec bin/pulsar bookie"
    environment:
      clusterName: test
      zkServers: zk1:2181,zk2:2181,zk3:2181
      numAddWorkerThreads: 8
      useHostNameAsBookieID: 'true'
    volumes:
      - pulsardatabk3:/pulsar/data
      - pulsarconfbk3:/pulsar/conf
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
    networks:
      pulsar:

  broker1:
    hostname: broker1
    container_name: broker1
    image: apachepulsar/pulsar-all:latest
    restart: on-failure
    command: >
      bash -c "bin/apply-config-from-env.py conf/broker.conf && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/watch-znode.py -z $$zookeeperServers -p /initialized-$$clusterName -w && \
               exec bin/pulsar broker"
    environment:
      clusterName: test
      zookeeperServers: zk1:2181,zk2:2181,zk3:2181
      configurationStore: zk1:2181,zk2:2181,zk3:2181
      webSocketServiceEnabled: 'false'
      functionsWorkerEnabled: 'false'
    volumes:
      - pulsardatabroker1:/pulsar/data
      - pulsarconfbroker1:/pulsar/conf
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
      - bk3
    networks:
      pulsar:

  broker2:
    hostname: broker2
    container_name: broker2
    image: apachepulsar/pulsar-all:latest
    restart: on-failure
    command: >
      bash -c "bin/apply-config-from-env.py conf/broker.conf && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/watch-znode.py -z $$zookeeperServers -p /initialized-$$clusterName -w && \
               exec bin/pulsar broker"
    environment:
      clusterName: test
      zookeeperServers: zk1:2181,zk2:2181,zk3:2181
      configurationStore: zk1:2181,zk2:2181,zk3:2181
      webSocketServiceEnabled: 'false'
      functionsWorkerEnabled: 'false'
    volumes:
      - pulsardatabroker2:/pulsar/data
      - pulsarconfbroker2:/pulsar/conf
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
      - bk3
      - broker1
    networks:
      pulsar:

  broker3:
    hostname: broker3
    container_name: broker3
    image: apachepulsar/pulsar-all:latest
    restart: on-failure
    command: >
      bash -c "bin/apply-config-from-env.py conf/broker.conf && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/watch-znode.py -z $$zookeeperServers -p /initialized-$$clusterName -w && \
               exec bin/pulsar broker"
    environment:
      clusterName: test
      zookeeperServers: zk1:2181,zk2:2181,zk3:2181
      configurationStore: zk1:2181,zk2:2181,zk3:2181
      webSocketServiceEnabled: 'false'
      functionsWorkerEnabled: 'false'
    volumes:
      - pulsardatabroker3:/pulsar/data
      - pulsarconfbroker3:/pulsar/conf
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
      - bk3
      - broker1
      - broker2
    networks:
      pulsar:

  proxy1:
    hostname: proxy1
    container_name: proxy1
    restart: on-failure
    image: apachepulsar/pulsar-all:latest
    command: >
      bash -c "bin/apply-config-from-env.py conf/proxy.conf && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/watch-znode.py -z $$zookeeperServers -p /initialized-$$clusterName -w && \
               exec bin/pulsar proxy"
    environment:
      clusterName: test
      zookeeperServers: zk1:2181,zk2:2181,zk3:2181
      configurationStoreServers: zk1:2181,zk2:2181,zk3:2181
      webSocketServiceEnabled: 'true'
      functionWorkerWebServiceURL: http://fnc1:6750
    volumes:
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    ports:
      - '6650:6650'
      - '8080:8080'
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
      - bk3
      - broker1
    networks:
      pulsar:

  websocket1:
    hostname: websocket1
    container_name: websocket1
    restart: on-failure
    image: apachepulsar/pulsar-all:latest
    command: >
      bash -c "bin/apply-config-from-env.py conf/websocket.conf && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/watch-znode.py -z $$zookeeperServers -p /initialized-$$clusterName -w && \
               exec bin/pulsar websocket"
    environment:
      clusterName: test
      zookeeperServers: zk1:2181,zk2:2181,zk3:2181
      configurationStoreServers: zk1:2181,zk2:2181,zk3:2181
    volumes:
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
      - bk3
      - broker1
      - proxy1
    networks:
      pulsar:

  sql1:
    hostname: sql1
    container_name: sql1
    image: apachepulsar/pulsar-all:latest
    restart: on-failure
    command: >
      bash -c "bin/apply-config-from-env-with-prefix.py SQL_PREFIX_ conf/presto/config.properties && \
               bin/apply-config-from-env-with-prefix.py SQL_PREFIX_ conf/presto/catalog/pulsar.properties && \
               bin/apply-config-from-env.py conf/pulsar_env.sh && \
               bin/watch-znode.py -z $$zookeeperServers -p /initialized-$$clusterName -w && \
               exec bin/pulsar sql-worker run"
    environment:
      clusterName: test
      node.id: 77152866-c932-4b91-93a9-4a885fe51089
      presto.version: 334
      zookeeperServers: zk1:2181,zk2:2181,zk3:2181
      configurationStoreServers: zk1:2181,zk2:2181,zk3:2181
      pulsar.zookeeper-uri: zk1:2181,zk2:2181,zk3:2181
      pulsar.broker-service-url: http://broker1:6650,broker2:6650,broker3:6650
      pulsar.web-service-url: http://broker1:8080,broker2:8080,broker3:8080
      coordinator: true
      node-scheduler.include-coordinator: false
      query.max-memory: 50GB
      query.max-memory-per-node: 10GB
      discovery-server.enabled: true
      discovery.uri: http://sql1:8081
      http-server.http.port: 8081
    volumes:
      - pulsardatasql1:/pulsar/data
      - pulsarconfsql1:/pulsar/conf
      - ./scripts/apply-config-from-env-with-prefix.py:/pulsar/bin/apply-config-from-env-with-prefix.py
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
      - bk3
      - broker1
      - proxy1
    ports:
      - '8081:8081'
    networks:
      pulsar:
  sql2:
    hostname: sql2
    container_name: sql2
    image: apachepulsar/pulsar-all:latest
    restart: on-failure
    command:
      - /bin/bash
      - -c
      - |
        echo 'coordinator=false' >> conf/presto/config.properties
        sed -i 's,^\(discovery-server.*\),#\1,' conf/presto/config.properties
        sed -i 's,^\(scheduler.*\),#\1,' conf/presto/config.properties
        bin/apply-config-from-env-with-prefix.py SQL_PREFIX_ conf/presto/catalog/pulsar.properties
        bin/apply-config-from-env-with-prefix.py SQL_PREFIX_ conf/presto/config.properties
        bin/apply-config-from-env.py conf/pulsar_env.sh
        bin/watch-znode.py -z $$zookeeperServers -p /initialized-$$clusterName -w
        exec bin/pulsar sql-worker run
    environment:
      clusterName: test
      presto.version: 334
      node.id: 7f2f2f24-84ea-48ae-a499-1de0957f05a5
      zookeeperServers: zk1:2181,zk2:2181,zk3:2181
      configurationStoreServers: zk1:2181,zk2:2181,zk3:2181
      pulsar.zookeeper-uri: zk1:2181,zk2:2181,zk3:2181
      coordinator: false
      http-server.http.port: 8081
      query.max-memory: 50GB
      query.max-memory-per-node: 10GB
      discovery.uri: http://sql1:8081
      pulsar.broker-service-url: http://broker1:6650,broker2:6650,broker3:6650
      pulsar.web-service-url: http://broker1:8080,broker2:8080,broker3:8080
    volumes:
      - pulsardatasql2:/pulsar/data
      - pulsarconfsql2:/pulsar/conf
      - ./scripts/apply-config-from-env-with-prefix.py:/pulsar/bin/apply-config-from-env-with-prefix.py
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
      - bk3
      - broker1
      - proxy1
      - sql1
    networks:
      pulsar:
  sql3:
    hostname: sql3
    container_name: sql3
    image: apachepulsar/pulsar-all:latest
    restart: on-failure
    command:
      - /bin/bash
      - -c
      - |
        echo 'coordinator=false' >> conf/presto/config.properties
        sed -i 's,^\(discovery-server.*\),#\1,' conf/presto/config.properties
        sed -i 's,^\(scheduler.*\),#\1,' conf/presto/config.properties
        bin/apply-config-from-env-with-prefix.py SQL_PREFIX_ conf/presto/catalog/pulsar.properties
        bin/apply-config-from-env-with-prefix.py SQL_PREFIX_ conf/presto/config.properties
        bin/apply-config-from-env.py conf/pulsar_env.sh
        bin/watch-znode.py -z $$zookeeperServers -p /initialized-$$clusterName -w
        exec bin/pulsar sql-worker run
    environment:
      clusterName: test
      presto.version: 334
      node.id: 0ff452f4-dd20-4f90-bcbd-9ae666edb958
      zookeeperServers: zk2:2181,zk1:2181,zk3:2181
      configurationStoreServers: zk2:2181,zk1:2181,zk3:2181
      pulsar.zookeeper-uri: zk2:2181,zk1:2181,zk3:2181
      coordinator: false
      #node-scheduler.include-coordinator: false
      http-server.http.port: 8081
      query.max-memory: 50GB
      query.max-memory-per-node: 10GB
      discovery.uri: http://sql1:8081
      pulsar.broker-service-url: http://broker2:6650,broker1:6650,broker3:6650
      pulsar.web-service-url: http://broker2:8080,broker1:8080,broker3:8080
    volumes:
      - pulsardatasql3:/pulsar/data
      - pulsarconfsql3:/pulsar/conf
      - ./scripts/apply-config-from-env-with-prefix.py:/pulsar/bin/apply-config-from-env-with-prefix.py
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
      - bk3
      - broker1
      - proxy1
      - sql1
    networks:
      pulsar:
  sql4:
    hostname: sql4
    container_name: sql4
    image: apachepulsar/pulsar-all:latest
    restart: on-failure
    command:
      - /bin/bash
      - -c
      - |
        echo 'coordinator=false' >> conf/presto/config.properties
        sed -i 's,^\(discovery-server.*\),#\1,' conf/presto/config.properties
        sed -i 's,^\(scheduler.*\),#\1,' conf/presto/config.properties
        bin/apply-config-from-env-with-prefix.py SQL_PREFIX_ conf/presto/catalog/pulsar.properties
        bin/apply-config-from-env-with-prefix.py SQL_PREFIX_ conf/presto/config.properties
        bin/apply-config-from-env.py conf/pulsar_env.sh
        bin/watch-znode.py -z $$zookeeperServers -p /initialized-$$clusterName -w
        exec bin/pulsar sql-worker run
    environment:
      clusterName: test
      presto.version: 334
      node.id: 1c0d1a15-a9d4-40f5-89b5-be686b4feeab
      zookeeperServers: zk3:2181,zk2:2181,zk1:2181
      configurationStoreServers: zk3:2181,zk2:2181,zk1:2181
      pulsar.zookeeper-uri: zk3:2181,zk2:2181,zk1:2181
      coordinator: false
      #node-scheduler.include-coordinator: false
      http-server.http.port: 8081
      query.max-memory: 50GB
      query.max-memory-per-node: 10GB
      discovery.uri: http://sql1:8081
      pulsar.broker-service-url: http://broker3:6650,broker2:6650,broker1:6650
      pulsar.web-service-url: http://broker3:8080,broker2:8080,broker1:8080
    volumes:
      - pulsardatasql4:/pulsar/data
      - pulsarconfsql4:/pulsar/conf
      - ./scripts/apply-config-from-env-with-prefix.py:/pulsar/bin/apply-config-from-env-with-prefix.py
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
      - bk3
      - broker1
      - proxy1
      - sql1
    networks:
      pulsar:

  fnc1:
    hostname: fnc1
    container_name: fnc1
    image: apachepulsar/pulsar-all:latest
    restart: on-failure
    command: >
      bash -c "bin/apply-config-from-env.py conf/client.conf && \
              bin/gen-yml-from-env.py conf/functions_worker.yml && \
              bin/apply-config-from-env.py conf/pulsar_env.sh && \
              bin/watch-znode.py -z $$zookeeperServers -p /initialized-$$clusterName -w && \
              exec bin/pulsar functions-worker"
    environment:
      clusterName: test
      zookeeperServers: zk1:2181,zk2:2181,zk3:2181
      # Requires PF_ prefix for some reason in the code
      PF_pulsarFunctionsCluster: test
      PF_workerId: fnc1
      # This setting does not appear to accept more than one host
      PF_configurationStoreServers: zk1:2181
      PF_pulsarServiceUrl: pulsar://proxy1:6650
      PF_pulsarWebServiceUrl: http://proxy1:8080
    volumes:
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
      - bk3
      - broker1
      - proxy1
    networks:
      pulsar:
  sql5:
    hostname: sql5
    container_name: sql5
    image: apachepulsar/pulsar-all:latest
    restart: on-failure
    command:
      - /bin/bash
      - -c
      - |
        echo 'coordinator=false' >> conf/presto/config.properties
        sed -i 's,^\(discovery-server.*\),#\1,' conf/presto/config.properties
        sed -i 's,^\(scheduler.*\),#\1,' conf/presto/config.properties
        bin/apply-config-from-env-with-prefix.py SQL_PREFIX_ conf/presto/catalog/pulsar.properties
        bin/apply-config-from-env-with-prefix.py SQL_PREFIX_ conf/presto/config.properties
        bin/apply-config-from-env.py conf/pulsar_env.sh
        bin/watch-znode.py -z $$zookeeperServers -p /initialized-$$clusterName -w
        exec bin/pulsar sql-worker run
    environment:
      clusterName: test
      presto.version: 334
      node.id: 9d8f2738-ab33-4ec5-a6df-2d3a17746030
      zookeeperServers: zk1:2181,zk2:2181,zk3:2181
      configurationStoreServers: zk1:2181,zk2:2181,zk3:2181
      pulsar.zookeeper-uri: zk1:2181,zk2:2181,zk3:2181
      coordinator: false
      #node-scheduler.include-coordinator: false
      http-server.http.port: 8081
      query.max-memory: 50GB
      query.max-memory-per-node: 10GB
      discovery.uri: http://sql1:8081
      pulsar.broker-service-url: http://broker1:6650,broker2:6650,broker3:6650
      pulsar.web-service-url: http://broker1:8080,broker2:8080,broker3:8080
    volumes:
      - pulsardatasql5:/pulsar/data
      - pulsarconfsql5:/pulsar/conf
      - ./scripts/apply-config-from-env-with-prefix.py:/pulsar/bin/apply-config-from-env-with-prefix.py
      - ./scripts/apply-config-from-env.py:/pulsar/bin/apply-config-from-env.py
    depends_on:
      - zk1
      - zk2
      - zk3
      - pulsar-init
      - bk1
      - bk2
      - bk3
      - broker1
      - proxy1
      - sql1
    networks:
      pulsar:

  setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
    user: '0'
    command: >
      bash -c '
        if [ x${ELASTIC_PASSWORD} == x ]; then
          echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
          exit 1;
        elif [ x${KIBANA_PASSWORD} == x ]; then
          echo "Set the KIBANA_PASSWORD environment variable in the .env file";
          exit 1;
        fi;
        if [ ! -f config/certs/ca.zip ]; then
          echo "Creating CA";
          bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;
          unzip config/certs/ca.zip -d config/certs;
        fi;
        if [ ! -f config/certs/certs.zip ]; then
          echo "Creating certs";
          echo -ne \
          "instances:\n"\
          "  - name: es01\n"\
          "    dns:\n"\
          "      - es01\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: es02\n"\
          "    dns:\n"\
          "      - es02\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: es03\n"\
          "    dns:\n"\
          "      - es03\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          > config/certs/instances.yml;
          bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
          unzip config/certs/certs.zip -d config/certs;
        fi;
        echo "Setting file permissions"
        chown -R root:root config/certs;
        find . -type d -exec chmod 750 \{\} \;;
        find . -type f -exec chmod 640 \{\} \;;
        echo "Waiting for Elasticsearch availability";
        until curl -s --cacert config/certs/ca/ca.crt http://es01:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
        echo "Setting kibana_system password";
        until curl -s -X POST --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" http://es01:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
        echo "All done!";
      '
    healthcheck:
      test: ['CMD-SHELL', '[ -f config/certs/es01/es01.crt ]']
      interval: 1s
      timeout: 5s
      retries: 120

  es01:
    depends_on:
      setup:
        condition: service_healthy
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - esdata01:/usr/share/elasticsearch/data
    ports:
      - ${ES_PORT}:9200
    environment:
      - node.name=es01
      - cluster.name=${CLUSTER_NAME}
      - ES_JAVA_OPTS=-Xms750m -Xmx750m
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es02,es03
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=false
      - xpack.security.http.ssl.key=certs/es01/es01.key
      - xpack.security.http.ssl.certificate=certs/es01/es01.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.http.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es01/es01.key
      - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
    mem_limit: ${MEM_LIMIT}
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          'CMD-SHELL',
          "curl -s http://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    networks:
      pulsar:

  es02:
    depends_on:
      - es01
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - esdata02:/usr/share/elasticsearch/data
    environment:
      - node.name=es02
      - cluster.name=${CLUSTER_NAME}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es01,es03
      - ES_JAVA_OPTS=-Xms750m -Xmx750m
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=false
      - xpack.security.http.ssl.key=certs/es02/es02.key
      - xpack.security.http.ssl.certificate=certs/es02/es02.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.http.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es02/es02.key
      - xpack.security.transport.ssl.certificate=certs/es02/es02.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
    mem_limit: ${MEM_LIMIT}
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          'CMD-SHELL',
          "curl -s http://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    networks:
      pulsar:

  es03:
    depends_on:
      - es02
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - esdata03:/usr/share/elasticsearch/data
    environment:
      - node.name=es03
      - ES_JAVA_OPTS=-Xms750m -Xmx750m
      - cluster.name=${CLUSTER_NAME}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es01,es02
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=false
      - xpack.security.http.ssl.key=certs/es03/es03.key
      - xpack.security.http.ssl.certificate=certs/es03/es03.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.http.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es03/es03.key
      - xpack.security.transport.ssl.certificate=certs/es03/es03.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
    mem_limit: ${MEM_LIMIT}
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          'CMD-SHELL',
          "curl -s http://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    networks:
      pulsar:

  kibana:
    depends_on:
      es01:
        condition: service_healthy
      es02:
        condition: service_healthy
      es03:
        condition: service_healthy
    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
    volumes:
      - certs:/usr/share/kibana/config/certs
      - kibanadata:/usr/share/kibana/data
    ports:
      - ${KIBANA_PORT}:5601
    environment:
      - SERVERNAME=kibana
      - ELASTICSEARCH_HOSTS=http://es01:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
    mem_limit: ${MEM_LIMIT}
    healthcheck:
      test:
        [
          'CMD-SHELL',
          "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    networks:
      pulsar:
volumes:
  pulsardatazk1:
  pulsarconfzk1:
  pulsardatazk2:
  pulsarconfzk2:
  pulsardatazk3:
  pulsarconfzk3:

  pulsardatabk1:
  pulsarconfbk1:
  pulsardatabk2:
  pulsarconfbk2:
  pulsardatabk3:
  pulsarconfbk3:

  pulsardatabroker1:
  pulsarconfbroker1:
  pulsardatabroker2:
  pulsarconfbroker2:
  pulsardatabroker3:
  pulsarconfbroker3:

  pulsardatasql1:
  pulsarconfsql1:
  pulsardatasql2:
  pulsarconfsql2:
  pulsardatasql3:
  pulsarconfsql3:
  pulsardatasql4:
  pulsarconfsql4:
  pulsardatasql5:
  pulsarconfsql5:
  certs:
    driver: local
  esdata01:
    driver: local
  esdata02:
    driver: local
  esdata03:
    driver: local
  kibanadata:
    driver: local

  # manager:
  #   hostname: manager
  #   container_name: manager
  #   image: apachepulsar/pulsar-manager:v0.1.0
  #   ports:
  #     - '9527:9527'
  #     - '7750:7750'
  #   depends_on:
  #     - broker1
  #   volumes:
  #     - './data/:/data'
  #   environment:
  #     REDIRECT_HOST: 'http://127.0.0.1'
  #     REDIRECT_PORT: '9527'
  #     DRIVER_CLASS_NAME: 'org.postgresql.Driver'
  #     URL: 'jdbc:postgresql://127.0.0.1:5432/pulsar_manager'
  #     USERNAME: 'pulsar'
  #     PASSWORD: 'pulsar'
  #     LOG_LEVEL: 'DEBUG'
  #   networks:
  #     pulsar:
